{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44881ae7-9f9c-4995-95f8-45b0b800323a",
   "metadata": {},
   "source": [
    "# 1. Multi-turn Conversational Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a3f5d-399a-4e89-b6b6-f1c803b10f6a",
   "metadata": {},
   "source": [
    "This notebook's content focuses on teaching you how to preprocess multiple chat data (multi-turn conversation) for the summarization task. Furthermore, it exposes learners to the fundamentals of LLMs in instruction tuning dataset and the NeMo-Run data format for fine-tuning. After completing a walkthrough of this notebook, learners will be able to preprocess data in a NeMo-Run acceptable format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94370aa-25a6-4acc-8245-6e72a873e4f1",
   "metadata": {},
   "source": [
    "### Fundamentals of Instruction Tuning Dataset\n",
    "\n",
    "After pre-training, instruction tuning (also known as supervised fine-tuning) is a crucial method for enhancing or unlocking specific abilities of LLMs. An example of such abilities is the ability to follow instructions. Based on the formatted instruction construction method, instruction tuning data can be categorized into three main types, namely, `NLP task datasets`, `daily chat datasets`, and `synthetic datasets`.\n",
    "\n",
    "- **NLP Task Datasets**. This kind of dataset is formatted based on collected NLP task datasets (e.g., text classification and summarization) with corresponding natural language task descriptions. Example include [P3](https://huggingface.co/datasets/bigscience/P3) and [FLAN](https://arxiv.org/pdf/2301.13688).\n",
    "- **Daily Chat Datasets**: Such a dataset is constructed based on real user conversations where queries are posed by humans, and responses are mainly generated by human labelers or LLMs. The conversation types include open-ended generation, question answering, brainstorming, and chatting. Examples include [ShareGPT](https://sharegpt.com/), [OpenAssistant](https://paperswithcode.com/dataset/oasst1), and [Dolly](https://github.com/databrickslabs/dolly).\n",
    "- **Synthetic Datasets**: These kinds of datasets are typically constructed by instructing LLMs, based on pre-defined guidance rules or methods. Examples include [Self-Instruct-52K](https://github.com/yizhongw/self-instruct), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [Baize](https://arxiv.org/abs/2304.01196).\n",
    "\n",
    "\n",
    "The [NeMo Curator](https://docs.nvidia.com/nemo/curator/latest/about/index.html#about-overview) is a typical source of constructing a synthetic dataset. It is an open-source, enterprise-grade platform for scalable, privacy-aware data curation across `text`, `image`, and `video` modalities. A comprehensive example on how to get started with text curation using the `NeMo Curator` can be found [here](https://docs.nvidia.com/nemo/curator/latest/get-started/text.html)\n",
    "\n",
    "\n",
    "#### Formatted Instance Construction\n",
    "\n",
    "Instruction tuning is the approach to fine-tuning pre-trained LLMs on a collection of formatted instances in the form of natural language, which is highly related to supervised fine-tuning and multi-task prompted training. To perform instruction tuning, we first need to collect or construct instruction-formatted instances. Then, we utilize these formatted instances to fine-tune LLMs in a supervised learning manner (e.g., training with sequence-to-sequence loss). An instruction-formatted instance consists of:\n",
    "- a task description (called an instruction),\n",
    "- an input,\n",
    "- the corresponding output, and\n",
    "- a small number of demonstrations (optional).\n",
    "\n",
    "The screenshot below illustrates three different methods for constructing instruction-formatted instances. You can read section 5 of [arXiv:2303.18223](https://arxiv.org/abs/2303.18223) paper for further details.\n",
    "\n",
    "\n",
    "<center><img src=\"images/formatted-instruction.png\" width=\"700px\" height=\"700px\"></center>\n",
    "<center>  An illustration of instance formatting and three different methods for constructing the instruction-formatted\n",
    "instances. <br/> <i>source: <a href=\"https://arxiv.org/abs/2303.18223\">arXiv:2303.18223 [cs.CL]</a></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6421e3-17ed-4023-930e-2c709855b2e2",
   "metadata": {},
   "source": [
    "### NeMo 2.0 Compatible Format For Custom Dataset\n",
    "\n",
    "To perform fine-tuning in [NeMo 2.0](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html), we must first transform the training dataset into a predefined compatible format. The NeMo 2.0 has a `FineTuningDataModule` base class for fine-tuning Large Language Models (LLMs) for supervised tasks. The task includes question answering, instruction tuning, function calling, summarization, etc. The `FineTuningDataModule` handles the data loading, preprocessing, and batch creation for training, validation, and testing phases. This class integrates with PyTorch Lightning’s LightningDataModule and NeMo’s SFT dataset classes (`GPTSFTDataset`, `GPTSFTChatDataset`, and `GPTSFTPackedDataset`).\n",
    "\n",
    "NeMo’s fine-tuning datasets are formatted as JSONL files. Each file contains lines of json-formatted text, and each line should contain a minimum of two keys, “input” and “output”. Additional keys can be added, and are returned by the data loader. An example is given below:\n",
    "\n",
    "```python\n",
    "{\"input\": \"This is the input/prompt/context/question for sample 1. Escape any double quotes like \\\"this\\\".\", \"output\": \"This is the output/answer/completion part of sample 1\"}\n",
    "{\"input\": \"This is the input/prompt/context/question for sample 2. Escape any double quotes like \\\"this\\\".\", \"output\": \"This is the output/answer/completion part of sample 2\"}\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "During training, by default, “input” and “output” are naively concatenated to be passed into the transformer model. Moreover, loss is only computed on the “output” tokens by default. These two behaviors can be customized with the dataset_kwargs field in the data module.\n",
    "\n",
    "```python\n",
    "\n",
    "FineTuningDataModule(\n",
    "    ...,\n",
    "    dataset_kwargs={\n",
    "        \"prompt_template\": \"Question: {input} Answer: {output}\",  # default is \"{input} {output}\" (naive concatenation)\n",
    "        \"answer_only_loss\": False,  # default is True (only calculate loss on answer/output)\n",
    "    }\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf602267-6c76-4651-94e8-7087e0464b0b",
   "metadata": {},
   "source": [
    "#### Using FineTuningDataModule with Preprocessed Data\n",
    "\n",
    "If you prefer to preprocess your dataset offline, you can use `FineTuningDataModule` directly by specifying the location of the preprocessed data following these two steps:\n",
    "- Create training, validation, and test files by preprocessing your raw data into a `.jsonl` format:\n",
    "\n",
    "```python\n",
    "your_dataset_root/\n",
    "    ├── training.jsonl\n",
    "    ├── validation.jsonl\n",
    "    └── test.jsonl\n",
    "\n",
    "```\n",
    "Each record in the `.jsonl` files should follow the format below:\n",
    "\n",
    "```python\n",
    "{\"input\": \"This is for sample 1. Escape any double quotes like \\\"this\\\".\", \"output\": \"This is the output for sample 1\"}\n",
    "{\"input\": \"This is for sample 2. Escape any double quotes like \\\"this\\\".\", \"output\": \"This is the output for sample 2\"}\n",
    "...\n",
    "\n",
    "```\n",
    "- Set up `FineTuningDataModule` to point to `dataset_root`, as well as any additional kwargs, if needed.\n",
    "\n",
    "```python\n",
    "FineTuningDataModule(\n",
    "    dataset_root=\"your_dataset_path\",\n",
    "    seq_length=512,\n",
    "    micro_batch_size=1,\n",
    "    global_batch_size=128,\n",
    "    dataset_kwargs={},\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Using ChatDataModule with Preprocessed Data\n",
    "\n",
    "You can also use the NeMo `ChatDataModule` dataset format for offline preprocessed data. The dataset format consists of three fields: `mask,` `system,` and `conversations.`\n",
    "\n",
    "- **Mask**: The role that needs to be masked out to prevent the role from participating in loss calculation.\n",
    "\n",
    "- **System**: System prompt.\n",
    "\n",
    "- **Conversations**: For each role, the conversation consists of two fields: `from` and `value`.\n",
    "\n",
    "```python\n",
    "{ \n",
    "    \"mask\": \"User\", \n",
    "    \"system\": \"\",\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"User\", \n",
    "            \"value\": \"You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\"\n",
    "        }, \n",
    "        {\n",
    "            \"from\": \"Assistant\", \n",
    "            \"value\": \"Of course. How long would you like your password to be? And would you like it to include symbols?\"\n",
    "        }, \n",
    "        {\n",
    "            \"from\": \"User\", \n",
    "            \"value\": \"I would like it to be 12 characters long and yes, please include symbols.\"\n",
    "        }, \n",
    "        {\n",
    "            \"from\": \"Assistant\", \n",
    "            \"value\": \"<TOOLCALL>[generate_password(length=12, include_symbols=True)]</TOOLCALL>\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "Also, the data must be formatted as `.jsonl` record file as shown below.\n",
    "\n",
    "```Python\n",
    "{\"system\": \"\", \"mask\": \"User\", \"conversations\": [{\"from\": \"User\", \"value\": \"You are an intelligent agent capable of invoking functions based on user queries. Given a question and a list of functions, your task is to identify and execute the appropriate functions. If no function is suitable, specify this. If required parameters are missing, indicate this as well. Return only the function call in the specified format. Here is the list of available functions in JSON format\\n<AVAILABLE_TOOLS>\\n[{\\\"name\\\": \\\"playlist\\\", \\\"description\\\": \\\"Fetch details and videos of a YouTube playlist using the provided playlist ID and optional parameters.\\\", \\\"parameters\\\": {\\\"is_id\\\": {\\\"description\\\":..\"}, {\"from\": \"Assistant\", \"value\": \"<TOOLCALL>[domain(domain_id=\\\"sample.info\\\"), playlist(is_id=\\\"PLghi\\\")]</TOOLCALL>\"}]}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773e5d3-22f1-4de5-939d-8ac3cab4f745",
   "metadata": {},
   "source": [
    "### DialogStudio Conversational Datasets\n",
    "\n",
    "[DialogStudio](https://github.com/salesforce/DialogStudio) is the most comprehensive and diverse collection of publicly available dialogue datasets, unified under a consistent format. By aggregating dialogues from various sources, DialogStudio facilitates holistic analysis and the development of models that are adaptable to a range of conversational scenarios.\n",
    " The collection spans an extensive range of domains, aspects, and tasks, encompassing several categories: `Open-Domain Dialogues`, `Task-Oriented Dialogues`, `Natural Language Understanding`, `Conversational Recommendation`, `Dialogue Summarization`, and `Knowledge-Grounded Dialogues`. Thus, it can provide support for research in both individual dialogue tasks and large-scale language pre-training.\n",
    "\n",
    "<center><img src=\"images/dialogstudio-dist.png\" width=\"700px\" height=\"700px\"></center>\n",
    "<center> (a) is the distribution of all datasets in DialogStudio. The outer and inner circle list names of datasets and\n",
    "the associated categories, respectively. (b) illustrates covered domains of Task-Oriented Dialogues in DialogStudio.. <br/> <i>source: <a href=\"https://arxiv.org/pdf/2307.10172\">arXiv:2307.10172 [cs.CL]</a></i></center> <br/>\n",
    "\n",
    "For learning purposes, we will focus on the `Dialogue Summarization` category for data preprocessing. The goal will be to preprocess some of the datasets (SAMSum and DialogSum) for a summarization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428bd88-809d-449c-b5c1-fe60acc05f32",
   "metadata": {},
   "source": [
    "#### Dialogue Summarization Dataset\n",
    "\n",
    "In this section, we will examine two datasets, including `SAMSum` and `DialogSum`. The preprocessing process of the SAMSum dataset will be exemplified in the remainder of this notebook, while the DialogSum dataset will be used for learners' activities to try out.\n",
    "\n",
    "#### SAMSum\n",
    "\n",
    "[SAMSum Corpus](https://aclanthology.org/D19-5409/) is a dataset with abstractive dialogue summaries. It contains high-quality chat dialogues that have been manually annotated with abstractive summarizations. The dataset contains 14,732 training samples, 818 validation samples, and 819 test samples. For our preprocessing process, we will consider 1000 training samples. The screenshot below displays the dictionary keys of the [JSON file](../data/SAMSum/train.json).\n",
    "\n",
    "<img src=\"images/SAMSum.png\" height=\"500px\" width=\"500px\">\n",
    "\n",
    "Each training sample consists of a key `SAMSum--train--xxx`, where `xxx` represents the indexes ranging from 1 to 1000. Let's explore the JSON file and display a sample by running the cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1bf4b-ee4a-45fb-9178-ec0754bb489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def read_json_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON format - {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db069fef-8d00-4a42-b447-21c4360fb3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = read_json_file(\"../data/SAMSum/train.json\")  # read SAMSum training JSON file\n",
    "data[\"SAMSum--train--10\"]  # display the content of the 10th sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1fa65b-fc81-4ee0-bd7e-4563e86153c3",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```text\n",
    "{'original dialog id': '13809912',\n",
    " 'dialog index': 10,\n",
    " 'original dialog info': {'summary': \"Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place.\"},\n",
    " 'log': [{'turn id': 1,\n",
    "   'user utterance': 'Do you want to go for date?',\n",
    "   'system response': 'Wow! You caught me out with this question Matt.',\n",
    "   'dialog history': '',\n",
    "   'original user side information': {'speaker': 'Matt'},\n",
    "   'original system side information': {'speaker': 'Agnes'}},\n",
    "  {'turn id': 2,\n",
    "   'user utterance': 'Why?',\n",
    "   'system response': \"I simply didn't expect this from you.\",\n",
    "   'dialog history': '<USER> Do you want to go for date? <SYSTEM> Wow! You caught me out with this question Matt.',\n",
    "   'original user side information': {'speaker': 'Matt'},\n",
    "   'original system side information': {'speaker': 'Agnes'}},\n",
    "  {'turn id': 3,\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495272aa-4972-4763-a338-3d7f8dc1e254",
   "metadata": {},
   "source": [
    "Each training sample consists of the following keys: `original dialog id`, `dialog index`, `original dialog info`, `log`, `original user side information`, and `original system side information`.  The `original dialog info` contains the dialog summary, while the `log` holds the dictionary list of dialog between a user and the system. To proceed, we write two functions:\n",
    "- First, to remove unwanted text, punctuation, and symbols.\n",
    "- Second, to extract the dialogue summary, dialogue log (conversation), and transform it into a single text. This will form a new sample ({conversation and summary}) for our processed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b7d47-09ab-487d-9616-85322d7249d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_umwanted_text_symbols(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\^[^ ]+\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@[^\\s]+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def transform_conversation(data):\n",
    "    transformed_text = \"\"\n",
    "    summary = data[\"original dialog info\"][\"summary\"]\n",
    "    for row in data[\"log\"]:\n",
    "        user = remove_umwanted_text_symbols(row[\"user utterance\"])\n",
    "        transformed_text += f\"user: {user.strip()}\\n\"\n",
    "        agent = remove_umwanted_text_symbols(row[\"system response\"])\n",
    "        transformed_text += f\"agent: {agent.strip()}\\n\"\n",
    "    return transformed_text, summary \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbbc9c-decd-410f-baca-927d977711e5",
   "metadata": {},
   "source": [
    "Run the cell below to display the processed version for the sample `SAMSum--train--10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c2478-206c-4c47-9edf-3a1186b890e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conversation, summary = transform_conversation(data[\"SAMSum--train--10\"])\n",
    "print(\"Conversation: \\n\", conversation)\n",
    "print(\"Summary: \", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349bdf7-b2bd-41ca-a50f-0f8949285352",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```text\n",
    "Conversation: \n",
    " user: Do you want to go for date?\n",
    "agent: Wow! You caught me out with this question Matt.\n",
    "user: Why?\n",
    "agent: I simply didn't expect this from you.\n",
    "user: Well, expect the unexpected.\n",
    "agent: Can I think about it?\n",
    "user: What is there to think about?\n",
    "agent: Well, I don't really know you.\n",
    "user: This is the perfect time to get to know eachother\n",
    "agent: Well that's true.\n",
    "...\n",
    "Summary:  Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb184021-4f2b-4dea-85b0-2ca8183347c6",
   "metadata": {},
   "source": [
    "One noticeable occurrence in the output is that the summary includes the names of the personalities involved in the dialog, which were replaced with `user` and `agent` in the conversation. This may result in a model fine-tuned on the dataset generating summaries that replace the names with 'user' and 'agent'. We can remedy this by extracting the speakers' names from the `original user side information` and the `original system side information`. Please run the cell below and confirm that the changes are effected in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff017b-eef0-436c-96b3-6f9f0dbe9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_conversation_speakers(data):\n",
    "    transformed_text = \"\"\n",
    "    summary = data[\"original dialog info\"][\"summary\"]\n",
    "    for row in data[\"log\"]:\n",
    "        user = remove_umwanted_text_symbols(row[\"original user side information\"][\"speaker\"])\n",
    "        system = remove_umwanted_text_symbols(row[\"original system side information\"][\"speaker\"])\n",
    "        user_utterance = remove_umwanted_text_symbols(row[\"user utterance\"])\n",
    "        transformed_text += f\"{user.strip()}: {user_utterance.strip()}\\n\"\n",
    "        system_response = remove_umwanted_text_symbols(row[\"system response\"])\n",
    "        transformed_text += f\"{system.strip()}: {system_response.strip()}\\n\"\n",
    "    return transformed_text, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70be139-a7fb-4b82-aec1-50d5bb08f915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conversation, summary = transform_conversation_speakers(data[\"SAMSum--train--10\"])\n",
    "print(\"Conversation: \\n\", conversation)\n",
    "print(\"Summary: \", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac771581-328f-473a-99fd-b01770c1f790",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```text\n",
    "Conversation: \n",
    "Matt: Do you want to go for date?\n",
    "Agnes: Wow! You caught me out with this question Matt.\n",
    "Matt: Why?\n",
    "Agnes: I simply didn't expect this from you.\n",
    "Matt: Well, expect the unexpected.\n",
    "Agnes: Can I think about it?\n",
    "Matt: What is there to think about?\n",
    "Agnes: Well, I don't really know you.\n",
    "...\n",
    "Summary:  Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf8dd2-09f0-42c8-838d-8313852e29fe",
   "metadata": {},
   "source": [
    "#### Task Description\n",
    "\n",
    "Task descriptions (also known as instructions) are an important component in constructing an instruction-following dataset. This helps a fine-tuned model know the task to be executed and improves its performance. For example, one can add `Write a summary of the conversation below` or `Below is a conversation between a user and an agent. Write a summary of their conversation.` There are several ways you can achieve this purpose. A common trick used to include the task description in the training data is to prefix it along with the input data. Below is a simple function to construct and prefix our summarization task description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f5f46-4a43-454e-8e41-667f7475cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_task_description(conversation):\n",
    "    return f\"\"\"### Instruction: Write a summary of the conversation below. ### Input: {conversation.strip()}\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7650d-5d68-4b57-8f30-b7dd42c33781",
   "metadata": {},
   "source": [
    "### Constructing Dataset in FineTuningDataModule Format\n",
    "\n",
    "As discussed previously, the FineTuningDataModule dataset construct is a dictionary of records containing both `input` and `output` fields. In our case, the input will be represented as a conversation prefix with a task description, and the output will denote the summary.\n",
    "\n",
    "```python\n",
    "\n",
    "new_row = {\n",
    "        \"input\": str(format_task_description(conversation)).strip(),\n",
    "        \"output\": str(summary).strip()       \n",
    "        }\n",
    "```\n",
    "\n",
    "Next, run the cells below to curate the new preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a52922-19ca-48ab-baf6-51dad93c2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preprocessed_dataset(dataset, type_data):\n",
    "    #length = len(dataset)\n",
    "    data_list = []\n",
    "    row_key = \"\"\n",
    "    if type_data == \"train\":\n",
    "        row_key = \"SAMSum--train--\"\n",
    "    elif type_data == \"val\":\n",
    "        row_key = \"SAMSum--val--\"\n",
    "    elif type_data == \"test\":\n",
    "        row_key = \"SAMSum--test--\"\n",
    "        \n",
    "    for index in range(1, len(dataset)+1):\n",
    "        key = row_key+str(index)\n",
    "        #print(key)\n",
    "        conversation, summary = transform_conversation_speakers(dataset[str(key)])\n",
    "        new_row = {\n",
    "        \"input\": str(format_task_description(conversation)).strip(),\n",
    "        \"output\": str(summary).strip()       \n",
    "        }\n",
    "        data_list.append(new_row)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435a08a-9d13-402a-b8df-bd33d21f7afb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training = read_json_file(\"../data/SAMSum/train.json\")  # read SAMSum training JSON file\n",
    "validation = read_json_file(\"../data/SAMSum/val.json\")  # read SAMSum validation JSON %%file\n",
    "test = read_json_file(\"../data/SAMSum/test.json\")  # read SAMSum test JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e76ad1a-b136-4b68-9198-0af0ccef1830",
   "metadata": {},
   "source": [
    "Now, let's process the training set and check the 10th sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b7f0c-6606-4ce7-aac7-510cdf1b816d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training = generate_preprocessed_dataset(training, \"train\")\n",
    "training[10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed67ab80-ad52-49d2-b2c2-382aee35e86a",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```python\n",
    "{'input': '### Instruction: Write a summary of the conversation below. ### Input: Lucas: Hey! How was your day?\\nDemi: Hey there!\\nDemi: It was pretty fine, actually, thank you!\\nDemi: I just got promoted! :D\\nLucas: Whoa! Great news!\\nLucas: Congratulations!\\nLucas: Such a success has to be celebrated.\\nDemi: I agree! :D\\nDemi: Tonight at Death & Co.?\\nLucas: Sure!\\nLucas: See you there at 10pm?\\nDemi: Yeah! See you there! :D',\n",
    " 'output': 'Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.'}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f47f6-dd4c-474b-8bf2-795c589df0dc",
   "metadata": {},
   "source": [
    "The next step is to process the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48322b9-8e51-466a-86e0-a42972330785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation = generate_preprocessed_dataset(validation, \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac345cf-554d-4d3a-8306-cb78ef865c0c",
   "metadata": {},
   "source": [
    "Finally, you run the cell below to process the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e15f3-f462-479b-a007-38ea7692e9f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = generate_preprocessed_dataset(test, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5dd67b-4d72-47ce-92c6-d480de24ac26",
   "metadata": {},
   "source": [
    "#### Save Data in JONSL Format\n",
    "\n",
    "NeMo 2.0 accepts JSONL data format. Therefore, we use `save_jsonl`, written below, to save our dataset in `.jsonl` format. Please note that each sample has to be written to a JSONL file as a dictionary record. Additionally, files must be named `training`, `validation`, and `test`. NeMo 2.0 uses this naming convention to identify the JSONL files during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a6cc2f-d7db-45f0-9779-328231ce8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def save_jsonl(filepath, data):\n",
    "    with open(filepath, \"w\") as write_file:\n",
    "         for row in tqdm(data):\n",
    "             write_file.write(json.dumps(row) + '\\n')\n",
    "         print(f\"{filepath} dataset saved \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6903e5-3a2f-4523-a763-4a5dcc3c95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"../data/SAMSum/finetune_module/training.jsonl\"\n",
    "validation_path = \"../data/SAMSum/finetune_module/validation.jsonl\"\n",
    "test_path = \"../data/SAMSum/finetune_module/test.jsonl\"\n",
    "\n",
    "save_jsonl(training_path, training)\n",
    "save_jsonl(validation_path, validation)\n",
    "save_jsonl(test_path, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a919a11-aa4a-4a88-b614-ec9e984fad4b",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```python\n",
    "100%|███████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 207751.94it/s]\n",
    "../data/SAMSum/finetune_module/training.jsonl dataset saved \n",
    "100%|█████████████████████████████████████████████████████████████████████████████| 818/818 [00:00<00:00, 206384.79it/s]\n",
    "../data/SAMSum/finetune_module/validation.jsonl dataset saved \n",
    "100%|█████████████████████████████████████████████████████████████████████████████| 819/819 [00:00<00:00, 208253.11it/s]\n",
    "../data/SAMSum/finetune_module/test.jsonl dataset saved \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b94e07-80ec-4f3a-a57d-5218d1bb13d2",
   "metadata": {},
   "source": [
    "Display the top 3 records from the [training set](../data/SAMSum/finetune_module/training.jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba2d66-c1d9-43fe-9ade-2589ab0f07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 ../data/SAMSum/finetune_module/training.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c9df7-ea4e-4c2d-9985-c279e61d5436",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```text\n",
    "{\"input\": \"### Instruction: Write a summary of the conversation below. ### Input: Amanda: I baked cookies. Do you want some?\\nJerry: Sure!\", \"output\": \"Amanda baked cookies and will bring Jerry some tomorrow.\"}\n",
    "{\"input\": \"### Instruction: Write a summary of the conversation below. ### Input: Olivia: Who are you voting for in this election?\\nOliver: Liberals as always.\\nOlivia: Me too!!\\nOliver: Great\", \"output\": \"Olivia and Olivier are voting for liberals in this election.\"}\n",
    "{\"input\": \"### Instruction: Write a summary of the conversation below. ### Input: Tim: Hi, what's up?\\nKim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\\nTim: What did you plan on doing?\\nKim: Oh you know, uni stuff and unfucking my room\\nKim: Maybe tomorrow I'll move my ass and do everything\\nKim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\\nTim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\\nTim: It really helps\\nKim: thanks, maybe I'll do that\\nTim: I also like using post-its in kaban style\", \"output\": \"Kim may try the pomodoro technique recommended by Tim to get more stuff done.\"}\n",
    "\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce91b8d-b5bc-48f2-86c8-7ed4d6f73269",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Lab Activity 1 (Optional)\n",
    "\n",
    "Following the preprocessing steps discussed to construct a dataset in `FineTuningDataModule` acceptable Format, apply this approach to preprocess `DialogSum` summarization dataset. The `DialogSum` is found in the location: `../data/DialogSum`.  \n",
    "\n",
    "*Tips*\n",
    "- *examine the [train](../data/DialogSum/train.json) and [validation](../data/DialogSum/val.json) sets for required information to extract*\n",
    "- *the fetch the files using `read_json_file` function and display a sample*\n",
    "- *Copy the `remove_umwanted_text_symbols` and `transform_conversation` functions. If applicable, modify the fields to suit your data extraction*\n",
    "- *use or modify the `format_task_description` function to define your task description*\n",
    "- *construct the `FineTuningDataModule` format using/modifying the `generate_preprocessed_dataset` function*\n",
    "- *save the preprocessed dataset (training.jsonl and validation.jsonl) to `../data/DialogSum/finetune_module/` using or modifying the `save_jsonl` function*\n",
    "  \n",
    "**Besides the given tips, you are free to explore an approach deemed best for you.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb6f220-252e-44e7-bf0b-07013e0c38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write activity solution here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce80804-99fa-49e0-a766-9889eda60413",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write activity solution here\n",
    "### You can also add more cells\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f4293-1c17-4861-ad4c-ab10cdea1272",
   "metadata": {},
   "source": [
    "---\n",
    "### Constructing dataset in ChatDataModule Format\n",
    "\n",
    "As previously discussed, the `ChatDataModule` dataset construct consists of three fields: `mask`, `system`, and `conversations`. To achieve this, we define a function to create  `ChatDataModule` fields and another to generate the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85cc0a2-d2bc-45d7-b396-30159850fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_conversation_speakers(data): ## This function is repeated to avoid scrolling up\n",
    "    transformed_text = \"\"\n",
    "    summary = data[\"original dialog info\"][\"summary\"]\n",
    "    for row in data[\"log\"]:\n",
    "        user = remove_umwanted_text_symbols(row[\"original user side information\"][\"speaker\"])\n",
    "        system = remove_umwanted_text_symbols(row[\"original system side information\"][\"speaker\"])\n",
    "        user_utterance = remove_umwanted_text_symbols(row[\"user utterance\"])\n",
    "        transformed_text += f\"{user.strip()}: {user_utterance.strip()}\\n\"\n",
    "        system_response = remove_umwanted_text_symbols(row[\"system response\"])\n",
    "        transformed_text += f\"{system.strip()}: {system_response.strip()}\\n\"\n",
    "    return transformed_text, summary\n",
    "\n",
    "def create_chatDataModule(conversation, summary):\n",
    "    record = {}\n",
    "    record[\"system\"] = \"\"\n",
    "    record[\"mask\"] = \"User\"\n",
    "    record[\"conversations\"] = [{\"from\":\"User\", \"value\": conversation}, {\"from\":\"Response\", \"value\": summary}]\n",
    "    return record\n",
    "\n",
    "\n",
    "def generate_conversation_chatDataModule(dataset, type_data):\n",
    "    data_list = []\n",
    "    row_key = \"\"\n",
    "    if type_data == \"train\":\n",
    "        row_key = \"SAMSum--train--\"\n",
    "    elif type_data == \"val\":\n",
    "        row_key = \"SAMSum--val--\"\n",
    "    elif type_data == \"test\":\n",
    "        row_key = \"SAMSum--test--\"\n",
    "    for index in range(1, len(dataset)+1):\n",
    "        key = row_key+str(index)\n",
    "        conversation, summary = transform_conversation_speakers(dataset[str(key)])\n",
    "        conversation = str(format_task_description(conversation)).strip()\n",
    "        record = create_chatDataModule(conversation, str(summary).strip())\n",
    "        data_list.append(record)\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e77e0-ea97-4a4c-ac55-ec431b395e96",
   "metadata": {},
   "source": [
    "Loading the JSON dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887fc473-5ce8-4bf2-af4a-3dd538672b8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chat_training = read_json_file(\"../data/SAMSum/train.json\")  # read SAMSum training JSON file\n",
    "chat_validation = read_json_file(\"../data/SAMSum/val.json\")  # read SAMSum validation JSON %%file\n",
    "chat_test = read_json_file(\"../data/SAMSum/test.json\")  # read SAMSum test JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46045f84-5615-4ef5-ad10-49ccdd4de8c8",
   "metadata": {},
   "source": [
    "Preprocessing the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f851f-bf8c-4c51-bbe0-3a6c2f13bdbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training = generate_conversation_chatDataModule(chat_training, \"train\")\n",
    "validation = generate_conversation_chatDataModule(chat_validation, \"val\")\n",
    "test = generate_conversation_chatDataModule(chat_test, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046f091-f132-4b93-bcf2-0b8b0c0c35fb",
   "metadata": {},
   "source": [
    "Let's display the 10th sample in the preprocessed training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee30aa4-ea1c-4224-ad90-fdbb07b0700c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training = generate_conversation_chatDataModule(chat_training, \"train\")\n",
    "training[10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35773f-da4d-4077-b675-876d3a5f14a7",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```python\n",
    "{'system': '',\n",
    " 'mask': 'User',\n",
    " 'conversations': [{'from': 'User',\n",
    "   'value': '### Instruction: Write a summary of the conversation below. ### Input: Lucas: Hey! How was your day?\\nDemi: Hey there!\\nDemi: It was pretty fine, actually, thank you!\\nDemi: I just got promoted! :D\\nLucas: Whoa! Great news!\\nLucas: Congratulations!\\nLucas: Such a success has to be celebrated.\\nDemi: I agree! :D\\nDemi: Tonight at Death & Co.?\\nLucas: Sure!\\nLucas: See you there at 10pm?\\nDemi: Yeah! See you there! :D'},\n",
    "  {'from': 'Response',\n",
    "   'value': 'Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.'}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08ee25-c44b-4165-ba2a-7b8075184d0c",
   "metadata": {},
   "source": [
    "#### Saving Preprocessed Data in JONSL Format\n",
    "\n",
    "Just as we did previously, the preprocessed dataset will be saved in NeMo 2.0's acceptable JSONL data format, and named `training.jsonl`, `validation.jsonl`, and `test.jsonl`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923cd0d2-448e-4a0c-a7bb-5d635484a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"../data/SAMSum/chat_module/training.jsonl\"\n",
    "validation_path = \"../data/SAMSum/chat_module/validation.jsonl\"\n",
    "test_path = \"../data/SAMSum/chat_module/test.jsonl\"\n",
    "\n",
    "save_jsonl(training_path, training)\n",
    "save_jsonl(validation_path, validation)\n",
    "save_jsonl(test_path, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebe6df5-aec5-480d-b60e-a4b46b84bc7b",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```python\n",
    "100%|███████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 156650.01it/s]\n",
    "../data/SAMSum/chat_module/training.jsonl dataset saved \n",
    "100%|█████████████████████████████████████████████████████████████████████████████| 818/818 [00:00<00:00, 163635.27it/s]\n",
    "../data/SAMSum/chat_module/validation.jsonl dataset saved \n",
    "100%|█████████████████████████████████████████████████████████████████████████████| 819/819 [00:00<00:00, 164030.89it/s]\n",
    "../data/SAMSum/chat_module/test.jsonl dataset saved \n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fc45d-541d-4de1-a5f3-97e6e4d4c67e",
   "metadata": {},
   "source": [
    "Now, let's display the top 3 records from the [training set](../data/SAMSum/chat_module/training.jsonl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c6997-f3d5-4183-80f9-f15cb211a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 ../data/SAMSum/chat_module/training.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb18df-6ad2-4cc4-9540-a524bf312953",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```python\n",
    "\n",
    "{\"system\": \"\", \"mask\": \"User\", \"conversations\": [{\"from\": \"User\", \"value\": \"### Instruction: Write a summary of the conversation below. ### Input: Amanda: I baked cookies. Do you want some?\\nJerry: Sure!\"}, {\"from\": \"Response\", \"value\": \"Amanda baked cookies and will bring Jerry some tomorrow.\"}]}\n",
    "{\"system\": \"\", \"mask\": \"User\", \"conversations\": [{\"from\": \"User\", \"value\": \"### Instruction: Write a summary of the conversation below. ### Input: Olivia: Who are you voting for in this election?\\nOliver: Liberals as always.\\nOlivia: Me too!!\\nOliver: Great\"}, {\"from\": \"Response\", \"value\": \"Olivia and Olivier are voting for liberals in this election.\"}]}\n",
    "{\"system\": \"\", \"mask\": \"User\", \"conversations\": [{\"from\": \"User\", \"value\": \"### Instruction: Write a summary of the conversation below. ### Input: Tim: Hi, what's up?\\nKim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\\nTim: What did you plan on doing?\\nKim: Oh you know, uni stuff and unfucking my room\\nKim: Maybe tomorrow I'll move my ass and do everything\\nKim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\\nTim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\\nTim: It really helps\\nKim: thanks, maybe I'll do that\\nTim: I also like using post-its in kaban style\"}, {\"from\": \"Response\", \"value\": \"Kim may try the pomodoro technique recommended by Tim to get more stuff done.\"}]}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd785d-f9e2-4726-9fb7-05a797aa9813",
   "metadata": {},
   "source": [
    "### Lab Activity 2 (Optional)\n",
    "\n",
    "Following the preprocessing steps exemplified to construct a dataset in `ChatDataModule` acceptable Format, apply this approach to preprocess `DialogSum` summarization dataset. The `DialogSum` is found in the location: `../data/DialogSum`.  \n",
    "\n",
    "*Tips*\n",
    "- *examine the [train](../data/DialogSum/train) and [validation](../data/DialogSum/val) sets for required information to extract*\n",
    "- *the fetch the files using `read_json_file` function and display a sample*\n",
    "- *Copy the `remove_umwanted_text_symbols` and `transform_conversation` functions. If applicable, modify the fields to suit your data extraction*\n",
    "- *use or modify the `format_task_description` function to define your task description*\n",
    "- *construct the `ChatDataModule` format using/modifying the `generate_conversation_chatDataModule` function*\n",
    "- *save the preprocessed dataset (training.jsonl and validation.jsonl) to `../data/DialogSum/chat_module/` using or modifying the `save_jsonl` function*\n",
    "  \n",
    "**Besides the given tips, you are free to explore an approach deemed best for you.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2631891e-b35e-4858-b91e-eb723027b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write activity solution here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef77962-4781-4a00-8534-2567c4820b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write activity solution here\n",
    "### You can also add more cells\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb3e1d-3bf9-4306-8970-c28e33ea4560",
   "metadata": {},
   "source": [
    "Now that we have learned how to preprocess a dialogue dataset, let's proceed to the next notebook and learn to fine-tune a `Llama-3.1-8B` model with the preprocessed dataset. Please click the link below.\n",
    "\n",
    "## <center><div style=\"text-align:center; color:#FF0000; border:3px solid red;height:80px;\"> <b><br/> [Next Notebook](finetuning.ipynb) </b> </div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee118af-7d8f-421e-abf2-5166ea13b445",
   "metadata": {},
   "source": [
    "---\n",
    "### References\n",
    "- [Overview of NeMo Curator](https://docs.nvidia.com/nemo/curator/latest/about/index.html#about-overview)\n",
    "- [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)\n",
    "- [https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html)\n",
    "- [https://github.com/salesforce/DialogStudio](https://github.com/salesforce/DialogStudio)\n",
    "\n",
    "### Licensing\n",
    "Copyright © 2025 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
