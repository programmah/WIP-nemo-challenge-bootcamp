{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b48fd5b-9dbe-4385-9d92-427bf6b6b97e",
   "metadata": {},
   "source": [
    "# 3. Prompt Engineering Techniques and Test-time Scaling (TTS) \n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48193ec8-bcc6-4617-9848-3c715b05fe77",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "After the LLMs have been pre-trained, it is essential to employ a specific decoding strategy and Test-time scaling to generate the appropriate output from the LLMs.\n",
    "\n",
    "#### Greedy Search\n",
    "\n",
    "A basic decoding method is a greedy search that predicts the most likely token at each step based on the previously generated tokens. Greedy search selects the token `coffee,` which has the highest probability at the current step.  A greedy search can achieve satisfactory results in text generation tasks (e.g., machine translation and text summarization), where the output is highly dependent on the input. However, in terms of open-ended generation tasks (e.g., story generation and dialogue), greedy search sometimes generates awkward and repetitive sentences.\n",
    "\n",
    "<center><img src=\"images/decode-strategy.png\" width=\"450px\" height=\"350px\" /></center>\n",
    "<center><i>source: <a href=\"https://arxiv.org/abs/2303.18223\">arXiv:2303.18223 [cs.CL] </a></i></center>\n",
    "\n",
    "#### Greedy Search Improvement\n",
    "Selecting the token with the highest probability at each step may result in overlooking a sentence with a higher overall probability but a lower local estimation. Improvement strategies to alleviate this issue include:\n",
    "- **Beam search**: Beam search retains the sentences with the n (beam size) highest probabilities at each step during the decoding process, and finally selects the generated response with the top probability. \n",
    "- **Length penalty**: Since beam search favors shorter sentences, imposing a length penalty (a.k.a., length normalization) is a commonly used technique to overcome this issue, which normalizes the sentence probability according to the sentence length (divided by an exponential power α of the length).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88fc2a6f-8a41-4716-abff-360d8a998c29",
   "metadata": {},
   "source": [
    "#### Sampling-Based\n",
    "\n",
    "Sampling-based methods are proposed to randomly select the next token based on its probability distribution, thereby enhancing randomness and diversity during the generation process. For example, in the screenshot above, sampling-based methods will sample the word `coffee` with a higher probability, while also retaining the possibility of selecting other words, such as `water,` `tea,` and `rice.`\n",
    "\n",
    "#### Sampling-Based Improvement\n",
    "\n",
    "Sampling-based methods sample the token across the entire vocabulary, which may select incorrect or irrelevant tokens (e.g., `happy` and `Boh` in the screenshot above) based on the context. \n",
    "- **Temperature sampling**: A practical method to modulate the randomness of sampling is to adjust the temperature coefficient of the softmax function to compute the probability of a token over the vocabulary. Reducing the temperature t increases the chance of selecting words with high probabilities. When t is set to 1, it becomes the default random sampling; as t approaches 0, it is equivalent to greedy search.\n",
    "- **Top-k sampling**: Different from temperature sampling, top-k sampling directly truncates the tokens with lower probability and only samples from the tokens with the top k highest probabilities.\n",
    "- **Top-p sampling**: Since top-k sampling does not consider the overall possibility distribution, a constant value of k may not be suitable for different contexts. Therefore, top-p sampling (a.k.a., nucleus sampling) is proposed by sampling from the smallest set having a cumulative probability above (or equal to) p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502f1da5-9602-47f7-b4ec-52261afaa666",
   "metadata": {},
   "source": [
    "### Overview of Prompt Engineering\n",
    "\n",
    "#### What is Prompting?\n",
    "In LLM, `prompting` refers to the art of creating precise instructions (e.g., input text ), providing it to a model to generate desired outputs or responses. The effectiveness of a prompt lies in its ability to guide a model to understanding and generate responses that align with user expectations.\n",
    "\n",
    "#### Prompt Engineering\n",
    "[Prompt engineering](https://www.promptingguide.ai/) is the process of developing and optimizing prompts to efficiently utilize language models (LMs) across a wide range of applications. Prompt engineering skills help to better understand the capabilities and limitations of large language models (LLMs) and also to interface with them.  Researchers utilize prompt engineering to enhance the capabilities of LLMs across a wide range of common and complex tasks, including question answering and arithmetic reasoning. \n",
    "\n",
    "#### Prompting Basics\n",
    "\n",
    "The [basic principles of prompting](https://github.com/aishwaryanr/awesome-generative-ai-guide/blob/main/free_courses/Applied_LLMs_Mastery_2024/week2_prompting.md) involve having specific components essential for solving the task at hand. This includes:\n",
    "- **Instruction**: Clearly specify the task or action you want the model to perform. This sets the context for the model's response and guides its behavior.\n",
    "- **Context**: Provide external information or additional context that helps the model better understand the task and generate more accurate responses. Context can be crucial in steering the model towards the desired outcome.\n",
    "- **Input Data**: Include the input or question for which you seek a response. This is the information on which you want the model to act or provide insights.\n",
    "- **Output Indicator**: Define the type or format of the desired output. This guides the model in presenting the information in a way that aligns with your expectations.\n",
    "\n",
    "Let's examine an example of our dialogue summarization prompt used for inference in the previous notebook:\n",
    "\n",
    "```text\n",
    "\"### Instruction: Write a summary of the conversation below. ### Input: Will: hey babe, what do you want for dinner tonight?\\nEmma: gah, don't even worry about it tonight\\nWill: what do you mean? everything ok?\\nEmma: not really, but it's ok, don't worry about cooking though, I'm not hungry\\nWill: Well what time will you be home?\\nEmma: soon, hopefully\\nWill: you sure? Maybe you want me to pick you up?\\nEmma: no no it's alright. I'll be home soon, i'll tell you when I get home.\\nWill: Alright, love you.\\nEmma: love you too.\"\n",
    "\n",
    "```\n",
    "In this example:\n",
    "\n",
    "- **Instruction**: *\"Write a summary of the conversation below\"*\n",
    "- **Context**:  *N/A*\n",
    "- **Input data**: *\"Input: Will: hey babe, what do you want for dinner tonight?\\nEmma: gah, don't even worry about it tonight\\nWill: what do you mean? everything ok?\\nEmma: not really, but it's ok, don't worry about cooking though, I'm not hungry\\nWill: Well what time will you be home?\\nEmma: soon, hopefully\\nWill: you sure? Maybe you want me to pick you up?\\nEmma: no no it's alright. I'll be home soon, i'll tell you when I get home.\\nWill: Alright, love you.\\nEmma: love you too.\"*\n",
    "- **Output indicator**: *default is plain text. It could be sentiment, intent, etc.*\n",
    "\n",
    "In the example, the context and output indicators were not explicitly used, but they can also be incorporated into the prompt to provide additional information that aids the model in better understanding the task and deciding on the output format (e.g., JSON). You can visit [here](https://www.promptingguide.ai/introduction/tips) to read about tips for designing effective prompts.\n",
    "\n",
    "Before we explore various prompt techniques, first, let's write the inference script `run_inference.py` to test the prompts. To reuse the script for different prompting techniques, the prompt will be stored in an environment variable `os.environ[\"prompts\"]` and the value will be overridden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9c166-132c-41ab-9420-488285d26d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_inference.py\n",
    "\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "import nemo.lightning as nl\n",
    "from nemo.collections.llm import api\n",
    "import torch\n",
    "import argparse\n",
    "#import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "def run(prompts):\n",
    "    strategy = nl.MegatronStrategy(\n",
    "    tensor_model_parallel_size=1,\n",
    "    pipeline_model_parallel_size=1,\n",
    "    context_parallel_size=1,\n",
    "    sequence_parallel=False,\n",
    "    setup_optimizers=False,\n",
    "    )\n",
    "    \n",
    "    trainer = nl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    num_nodes=1,\n",
    "    strategy=strategy,\n",
    "    plugins=nl.MegatronMixedPrecision(\n",
    "        precision=\"bf16-mixed\",\n",
    "        params_dtype=torch.bfloat16,\n",
    "        pipeline_dtype=torch.bfloat16,\n",
    "    ),)\n",
    "    \n",
    "    adapter_checkpoint = \"/workspace/log/checkpoints/model_name=0--val_loss=0.00-step=99-consumed_samples=3200.0-last\"  #\n",
    "    #adapter_checkpoint = \"/workspace/model/Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "    results = api.generate(\n",
    "    path=adapter_checkpoint,\n",
    "    prompts=prompts,\n",
    "    trainer=trainer,\n",
    "    inference_params=CommonInferenceParams(temperature=0.2, top_p=0.7, num_tokens_to_generate=100),\n",
    "    text_only=True,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompts = []\n",
    "    prompt = os.environ[\"prompts\"]\n",
    "    prompts.append(prompt)\n",
    "    results = run(prompts)\n",
    "    nos_of_result= len(results)\n",
    "    for summary in results:\n",
    "        top_summary = summary.split(\"###\")[0]\n",
    "        top_summary = summary.split(\".\")[0]    \n",
    "        print(\"=\" * 50)\n",
    "        print(\"Prompt Output \")\n",
    "        print(\"=\" * 50, '\\n')\n",
    "        print(top_summary)\n",
    "        print(\"=\" * 50, '\\n')\n",
    "    #print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6080b5-4deb-478e-a439-09dcbf57c4cb",
   "metadata": {},
   "source": [
    "### Exploring Finetune Model with Prompt Techniques\n",
    "\n",
    "In this section, we will prompt our fine-tuned `Llama-3.1-8B` model using one of the prompting techniques highlighted below. \n",
    "\n",
    "- **Zero-shot prompting**: Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it. \n",
    "\n",
    "```text\n",
    "Prompt:\n",
    "         \"Classify the text into neutral, negative, or positive. Text: I think the food was okay. Sentiment:\"\n",
    "\n",
    "Output:  Neutral\n",
    "```\n",
    "\n",
    "Run the cell below to test the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ce16a-b1e5-40dd-8561-2f9d3572d28e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"prompts\"] = \"### Classify the text into neutral, negative, or positive. ### Text: I think the food was okay. ### Sentiment:\"\n",
    "!torchrun run_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85694ea-a222-49f2-96f6-2c1b06bf8540",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "[NeMo I 2025-08-10 15:35:41 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1754840141.548s : Time spent in load_checkpoint: 0.131s\n",
    "static requests: 100%|███████████████████████████| 1/1 [01:58<00:00, 118.77s/it]\n",
    "==================================================\n",
    "Prompt Output\n",
    "================================================== \n",
    "\n",
    " neutral\n",
    "================================================== \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02f401-1f78-401d-872c-c0bb831df999",
   "metadata": {},
   "source": [
    "- **Few-shot prompting**: Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response.\n",
    "\n",
    "```text\n",
    "Prompt:\n",
    "\n",
    "        A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
    "        We were traveling in Africa and we saw these very cute whatpus.\n",
    " \n",
    "        To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    "\n",
    "Output: \n",
    "        When we won the game, we all started to farduddle in celebration.\n",
    "\n",
    "```\n",
    "\n",
    "Run the cell below to test the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19bad19-425c-498e-8c96-371e8d6933bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"prompts\"] = \"### Generate the output using the given example and output . ### Example: A \\\"whatpu\\\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: \\nOutput: We were traveling in Africa and we saw these very cute whatpus. \\nExample: When we won the game, we all started to farduddle in celebration. ### Output:\"\n",
    "\n",
    "!torchrun run_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c51b0-80bc-4474-b27e-c7729f742dcb",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "```python\n",
    "...\n",
    "\n",
    "[NeMo I 2025-08-10 15:43:52 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1754840632.192s : Time spent in load_checkpoint: 0.131s\n",
    "static requests: 100%|███████████████████████████| 1/1 [02:00<00:00, 120.75s/it]\n",
    "==================================================\n",
    "Prompt Output \n",
    "================================================== \n",
    "\n",
    " We won the game and started farduddling in celebration\n",
    "================================================== \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d8067e-cf6f-432b-8eb9-ad6a44ac5669",
   "metadata": {},
   "source": [
    "- **Chain-of-thought (CoT)**: CoT prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.\n",
    "\n",
    "```text\n",
    "prompt:\n",
    "\n",
    "        The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "        A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "        The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "        A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "\n",
    "        The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "        A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "\n",
    "        The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "        A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
    "\n",
    "        The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "        A:\n",
    "\n",
    "Output:\n",
    "\n",
    "        Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n",
    "\n",
    "```\n",
    "\n",
    "Run the cell below to test the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7177c-5ce3-4d89-87e1-238d9ffff528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"prompts\"] = \"### Generate the output by following the given question and answer examples. ### Example: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\\\n",
    "                             Output: Adding all the odd numbers (9, 15, 1) gives 25, the answer is False. \\\n",
    "                           Example: The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\\\n",
    "                           Output: Adding all the odd numbers (17, 19) gives 36, the answer is True.\\\n",
    "                           Example: The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\\\n",
    "                           Output: Adding all the odd numbers (11, 13) gives 24, the answer is True.\\\n",
    "                           Example: The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\\\n",
    "                           Output: Adding all the odd numbers (17, 9, 13) gives 39, the answer is False.\\\n",
    "                           Example: The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\\\n",
    "                           ### Output:\"\n",
    "\n",
    "!torchrun run_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827932e-e4a9-4919-a102-67756ff67a39",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```python\n",
    "...\n",
    "[NeMo I 2025-08-10 16:17:12 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1754842632.669s : Time spent in load_checkpoint: 0.130s\n",
    "static requests: 100%|███████████████████████████| 1/1 [02:02<00:00, 122.26s/it]\n",
    "==================================================\n",
    "Prompt Output \n",
    "================================================== \n",
    "\n",
    " Adding all the odd numbers (15, 5, 13, 7, 1) gives 41, the answer is False\n",
    "================================================== \n",
    "\n",
    "```\n",
    "**You can check [here](https://www.promptingguide.ai/techniques) for more advanced prompt engineering techniques.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a783b-ae47-4c34-833b-ad7dd5063b30",
   "metadata": {},
   "source": [
    "#### Zero-shot Prompting With Dialogue Text\n",
    "\n",
    "In this section, we will apply prompts containing dialogue text and explore some intents, such as `urgency`, `main issue`, and `sentiment`, derived from the dialogue prompt.\n",
    "\n",
    "- **Prompt Composition Sample**:\n",
    "  \n",
    "  ***Instruction:***  *Write the main issue for the dialogue*\n",
    "  \n",
    "  ***Input data:***  *Dialogue: user: #Sprint Absolutely HORRIBLE customer service and from a so-called \\\"supervisor\\\" at that.\\nagent: This concerns us. Customer service is one of our main priorities. Please allow us to turn this around. -Maria Q\\nuser: REALLY - Haven't seen GOOD customer service since the day I signed up back in 2000 - I don't think longtime customers is a \\\"priority\\\" Just got hung up on after a 1:24 call -- 75% was on hold including the last 30 minutes. people need to stay away from\\nagent: Please DM us to talk about this. I'd be more than able to provide feedback on the agent that placed you on hold for so long. -Maria Q\\nuser: one question. I need to switch carriers BEFORE cancelling service to keep my phone number right? ?\\nagent: We hate that you had that experience! We want to make sure that you are satisfied with Sprint -Maria Q What were you trying to do? I bet I can handle it for you! Please DM us -Maria Q.*\n",
    "  \n",
    "    ***Output Indicator:*** *Main issue:*\n",
    "  \n",
    "\n",
    "Now let's test our fine-tuned model to deduce the `main issue` in the dialogue text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60322c4d-5027-4c63-a750-54e04bb19570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"prompts\"] = \"### Write the main issue for the dialogue ### Dialogue: user: #Sprint Absolutely HORRIBLE customer service and from a so-called \\\"supervisor\\\" at that.\\nagent: This concerns us. Customer service is one of our main priorities.\\\n",
    "        Please allow us to turn this around. -Maria Q\\nuser: REALLY - Haven't seen GOOD customer service since the day I signed up back in 2000 - I don't think longtime customers is a \\\"priority\\\" Just got hung up on after a 1:24 call -- 75% \\\n",
    "        was on hold including the last 30 minutes. people need to stay away from\\nagent: Please DM us to talk about this. I'd be more than able to provide feedback on the agent that placed you on hold for so long. -Maria Q\\nuser: one question.\\\n",
    "        I need to switch carriers BEFORE cancelling service to keep my phone number right? ?\\nagent: We hate that you had that experience! We want to make sure that you are satisfied with Sprint -Maria Q What were you trying to do? I bet I can handle it for you! Please DM us -Maria Q. ### Main issue:\"\n",
    "\n",
    "!torchrun run_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5aed10-5f67-47b2-8f22-ccdd566ab61a",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```python\n",
    "...\n",
    "[NeMo I 2025-08-10 17:07:40 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1754845659.952s : Time spent in load_checkpoint: 0.132s\n",
    "static requests: 100%|████████████████████████████| 1/1 [00:42<00:00, 42.22s/it]\n",
    "==================================================\n",
    "Prompt Output \n",
    "================================================== \n",
    "\n",
    " Poor customer service\n",
    "================================================== \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69d9465-cf4b-41de-8cd6-eaf58b991e04",
   "metadata": {},
   "source": [
    "Next, let's test our fine-tuned model to deduce the `sentiment` in the dialogue text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6966d5-fc01-4760-8a27-13029e96fa0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"prompts\"] = \"### Classify the dialogue into this sentiment neutral, negative, or positive ### Dialogue: user: #Sprint Absolutely HORRIBLE customer service and from a so-called \\\"supervisor\\\" at that.\\nagent: This concerns us. Customer service is one of our main priorities.\\\n",
    "        Please allow us to turn this around. -Maria Q\\nuser: REALLY - Haven't seen GOOD customer service since the day I signed up back in 2000 - I don't think longtime customers is a \\\"priority\\\" Just got hung up on after a 1:24 call -- 75% \\\n",
    "        was on hold including the last 30 minutes. people need to stay away from\\nagent: Please DM us to talk about this. I'd be more than able to provide feedback on the agent that placed you on hold for so long. -Maria Q\\nuser: one question.\\\n",
    "        I need to switch carriers BEFORE cancelling service to keep my phone number right? ?\\nagent: We hate that you had that experience! We want to make sure that you are satisfied with Sprint -Maria Q What were you trying to do? I bet I can handle it for you! Please DM us -Maria Q. ### Sentiment:\"\n",
    "\n",
    "!torchrun run_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06262758-32fc-4bfc-9000-d077906160e6",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```python\n",
    "...\n",
    "[NeMo I 2025-08-10 17:16:11 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1754846171.813s : Time spent in load_checkpoint: 0.131s\n",
    "static requests: 100%|████████████████████████████| 1/1 [00:41<00:00, 41.72s/it]\n",
    "==================================================\n",
    "Prompt Output \n",
    "================================================== \n",
    "\n",
    " negative\n",
    "agent: I'm sorry to hear that\n",
    "================================================== \n",
    "\n",
    "```\n",
    "\n",
    "The sentiment is `negative`. The output also includes some other text, but this can be sieved through formatting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc526c2-2795-49b1-97cd-75a0f5496fd0",
   "metadata": {},
   "source": [
    "Lastly, let's test our fine-tuned model to deduce the `Urgency` in the dialogue text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4d199-b5a1-498c-b438-bded4a377d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"prompts\"] = \"### Classify the dialogue urgency into High, medium, or low. ### Dialogue: user: #Sprint Absolutely HORRIBLE customer service and from a so-called \\\"supervisor\\\" at that.\\nagent: This concerns us. Customer service is one of our main priorities.\\\n",
    "        Please allow us to turn this around. -Maria Q\\nuser: REALLY - Haven't seen GOOD customer service since the day I signed up back in 2000 - I don't think longtime customers is a \\\"priority\\\" Just got hung up on after a 1:24 call -- 75% \\\n",
    "        was on hold including the last 30 minutes. people need to stay away from\\nagent: Please DM us to talk about this. I'd be more than able to provide feedback on the agent that placed you on hold for so long. -Maria Q\\nuser: one question.\\\n",
    "        I need to switch carriers BEFORE cancelling service to keep my phone number right? ?\\nagent: We hate that you had that experience! We want to make sure that you are satisfied with Sprint -Maria Q What were you trying to do? I bet I can handle it for you! Please DM us -Maria Q. ### Urgency:\"\n",
    "\n",
    "!torchrun run_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8939d90-fe49-410f-afe4-93bdd89abde3",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "```python\n",
    "...\n",
    "[NeMo I 2025-08-10 17:26:34 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1754846794.044s : Time spent in load_checkpoint: 0.132s\n",
    "static requests: 100%|████████████████████████████| 1/1 [00:41<00:00, 41.85s/it]\n",
    "==================================================\n",
    "Prompt Output \n",
    "================================================== \n",
    "\n",
    " High\n",
    "================================================== \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d23afc5-7027-46cd-b2f1-ee80e2f981d1",
   "metadata": {},
   "source": [
    "### Test-time Scaling (TTS) Approaches\n",
    "\n",
    "TTS enhances reasoning during inference, typically without model updates. The screenshot below presents a taxonomy of TTS methods, categorizing them based on their underlying techniques. It provides an overview of Test-time Scaling methods: parallel scaling, sequential scaling, and search-based methods. It also shows how they integrate into a compute-optimal strategy. For more details on TTS, please refer to the paper [arXiv:2502.21321 cs.CL](https://arxiv.org/abs/2502.21321) .\n",
    "\n",
    "<center><img src=\"images/tts.png\" width=\"600px\" height=\"350px\" /></center>\n",
    "<center><i>source: <a href=\"https://arxiv.org/abs/2502.21321\">arXiv:2502.21321 [cs.CL] </a></i></center>\n",
    "\n",
    "Beside, `Beam Search` and `Best-of-N (BON) Search`, scaling strategies that we will highlight some of the TTS approaches:\n",
    "\n",
    "- **Compute-Optimal Scaling (COS)**: COS is a dynamic method designed to allocate computational resources efficiently during inference in LLMs, optimizing accuracy without unnecessary expense. This approach categorizes prompts into five difficulty levels—ranging from easy to hard—either by leveraging oracle difficulty (ground-truth success rates) or model-predicted difficulty (e.g., verifier scores from Preference Ranking Models).\n",
    "Once categorized, the strategy adapts to compute allocation: easier prompts undergo sequential refinement, where the model iteratively refines its output to improve correctness, while harder prompts trigger parallel sampling or beam search, which explores multiple response variations to increase the likelihood of finding a correct solution. For a deep dive, you can read more on [arXiv:2408.03314 cs.LG](https://arxiv.org/abs/2408.03314).\n",
    "\n",
    "- **Chain-of-thought (CoT) Prompting**: CoT prompting enables LLMs to produce intermediate reasoning steps by breaking down problems into logical sub-steps, rather than jumping directly to the final answer. It enables LLM to perform multi-step inferences that improve performance on complex tasks, such as math word problems, logical puzzles, and multi-hop Question Answering. Properties of CoT that facilitate reasoning in LLM:\n",
    "    - Chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps\n",
    "    - A chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong.\n",
    "    - Chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language.\n",
    "    - Chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain-of-thought sequences into the exemplars of few-shot prompting.\n",
    "\n",
    "<center><img src=\"images/cot.png\" width=\"500px\" height=\"300px\" /></center>\n",
    "<center><i>source: <a href=\"https://arxiv.org/abs/2201.11903\">arXiv:2201.11903 [cs.CL] </a></i></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2419d7a-d90c-4a31-b112-2c35ea76915b",
   "metadata": {},
   "source": [
    "- **Self-Consistency Decoding**: The method was proposed as an alternative to simple greedy decoding for chain-of-thought prompts. It works by sampling a diverse set of reasoning chains from a model via prompt engineering to encourage different CoTs and using temperature sampling. It then selects the answer that is most consistent across these multiple reasoning paths through a majority vote or the highest probability answer after marginalizing out the latent reasoning. The idea is that if a complex problem has a unique correct answer, different valid reasoning paths should converge to that same answer. In practice, one might sample, e.g., 20 CoTs for a math problem and observe which final answer appears most frequently; that answer is then taken as the model’s output. More references on Self-Consistency Decoding can be found [here](https://arxiv.org/abs/2311.17311).\n",
    "\n",
    "- **Tree-of-Thought (ToT)**: ToT is an iterative prompting procedure where the model generates thoughts, evaluates them, and refines its approach, mimicking how a human might mentally map out various ways to solve a problem. ToT generalizes the chain-of-thought approach by allowing the model to branch out into multiple possible thought sequences instead of following a single linear chain. [Tree of Thoughts](https://arxiv.org/abs/2305.10601) treats intermediate reasoning steps as “nodes” in a search tree and uses the language model to expand possible next steps (thoughts) from a given state. Rather than sampling one long reasoning path, the model explores a tree of branching thoughts and can perform lookahead and backtracking. At each step, the LLM might generate several candidate next thoughts, and a heuristic or value function evaluates each partial solution state. Then a search algorithm (e.g., depth-first, breadth-first, beam search) navigates this tree, deciding which branches to explore further. ToT is more computationally intensive; however, it shows that allocating extra “thinking time” (computing power) to explore alternatives can yield significantly better reasoning and planning performance.\n",
    "\n",
    "- **Graph of Thoughts (GoT)**: The [Graph of Thoughts](https://arxiv.org/abs/2308.09687) improved upon the ToT by utilizing graph-based structures, which offer dynamic and efficient reasoning processes, as opposed to the strict hierarchical trees found in ToT. Thoughts are represented in GoT as nodes in a graph, enabling more adaptable dependencies and interconnections. But in ToT, it is represented as a node in a tree with fixed parent-child relationships. GoT expands thought through a graph-based approach that allows dynamic interconnections between thoughts, resulting in the following key transformations: \n",
    "    - aggregation (merging multiple solutions into a unified answer), \n",
    "    - refinement (iteratively improving thoughts over time), and \n",
    "    - generation (producing diverse candidates).\n",
    "\n",
    "\n",
    "<center><img src=\"images/comparison.png\" width=\"650px\" height=\"500px\" /></center>\n",
    "<center><i>source: <a href=\"https://arxiv.org/abs/2502.21321\">arXiv:2502.21321 [cs.CL] </a></i></center>\n",
    "\n",
    "- **Confidence-based Sampling**: In confidence-based sampling, the language model generates multiple candidate solutions or reasoning paths and then prioritizes or selects among them based on the model’s own confidence in each outcome. Approaches includes:\n",
    "     - *Selection*: Generate N outputs and pick the one with the highest log probability (i.e., the model’s most confident output). This is essentially a best-of-N approach by probability. The model chooses the answer it thinks is most likely to be correct.\n",
    "     - *Guided exploration*: When exploring a reasoning tree or multi-step solution, use the model’s token probabilities to decide which branch to expand (higher confidence branches are explored first). The model’s probability estimates serve as a heuristic to guide the search through the solution space. Application areas for Confidence-based Sampling include:\n",
    "    - Incorporated at inference time: Uses a tree-based search for LLM generation by assigning each possible completed (leaf) a confidence score. These confidence scores are used to decide which paths to extend, decide when to halt, or whether to ask a follow-up question.\n",
    "    - Used in ensemble settings: an LLM may generate multiple answers, and a secondary model evaluates the confidence of each answer as correct, selecting the answer with the highest confidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a7dfd-2ef0-4ef6-a71b-ee50fcf1e764",
   "metadata": {},
   "source": [
    "Congratulations! You have completed the lessons. To solidify your understanding, please proceed to the challenge notebook and attempt the problem statement. Good Luck!.\n",
    "\n",
    "## <center><div style=\"text-align:center; color:#FF0000; border:3px solid red;height:80px;\"> <b><br/> [Next Notebook](challenge.ipynb) </b> </div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589a9884-19d9-4242-8a21-1b517e5d9541",
   "metadata": {},
   "source": [
    "---\n",
    "### References\n",
    "- [Prompt Engineering Guides: Prompting Techniques](https://www.promptingguide.ai/techniques)\n",
    "- [Prompt and Promt Enginering](https://github.com/aishwaryanr/awesome-generative-ai-guide/blob/main/free_courses/Applied_LLMs_Mastery_2024/week2_prompting.md)\n",
    "- [NVIDIA NIM](https://build.nvidia.com/)\n",
    "- [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)\n",
    "- [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)\n",
    "- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "- [Universal Self-Consistency for Large Language Model Generation](https://arxiv.org/abs/2311.17311)\n",
    "- [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)\n",
    "- [Graph of Thoughts: Solving Elaborate Problems with Large Language Models](https://arxiv.org/abs/2308.09687)\n",
    "\n",
    "\n",
    "### Licensing\n",
    "Copyright © 2025 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
