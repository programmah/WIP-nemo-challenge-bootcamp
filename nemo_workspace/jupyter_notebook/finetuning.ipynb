{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d5d8b8-816c-4067-aa41-3edad38b43bb",
   "metadata": {},
   "source": [
    "## Building A Text Summarization Model With NeMo-Run\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4c48c2-d3b4-4e33-a79c-b55abd81ab31",
   "metadata": {},
   "source": [
    "The notebook content focuses on teaching learners how to fine-tune an SOTA model for a summarization task using NeMo-Run. The rest of the notebook will expose learners to the NeMo Framework, an overview of NeMo-Run, NeMo fine-tuning models, and LoRA. Upon completing this content, learners will be able to fine-tune an SOTA model for the summarization task and perform inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca32222-1cfc-4ed8-bbb7-027d4a62df1d",
   "metadata": {},
   "source": [
    "### Overview of NeMo Framework\n",
    "\n",
    "[NVIDIA NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html) is a scalable and cloud-native generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (e.g., Automatic Speech Recognition and Text-to-Speech). It provides end-to-end support for developing Large Language Models (LLMs) and provides the flexibility to be used on-premises, in a data center, or with your preferred cloud provider. It also supports execution on `SLURM` or `Kubernetes-enabled` environments. NeMo Framework provides tools for efficient training and customization of LLM models. It includes default configurations for setting up a compute cluster, downloading data, and adjusting model hyperparameters, which can be customized to train on new datasets and models. In addition to pre-training, NeMo supports both [Supervised Fine-Tuning (SFT)](https://huggingface.co/learn/llm-course/en/chapter11/3) and [Parameter-Efficient Fine-Tuning (PEFT)](https://arxiv.org/pdf/2312.12148) techniques, such as [LoRA](https://arxiv.org/pdf/2106.09685), [Ptuning](https://arxiv.org/pdf/2110.07602), and others.\n",
    "\n",
    "<center><img src=\"images/NeMo-arch.png\" width=\"900\" height=\"900\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c0000-e7c4-480a-a206-e38491257704",
   "metadata": {},
   "source": [
    "There are two options available for launching the training process in NeMo: using the `NeMo 2.0 API interface` or with [NeMo Run](https://github.com/NVIDIA-NeMo/Run). For this notebook, our focus will be on using `NeMo Run`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfc9be-bcc0-404c-bf63-6c44731d85b1",
   "metadata": {},
   "source": [
    "#### NeMo Supported Models for Finetuning LLM\n",
    "\n",
    "NeMo comes equipped with a CLI that allows you to launch experiments locally or on a remote cluster. Through the CLI, you can check the list of fine-tune models. Run the cell below to view the list of nemo llm finetune models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db51ac3-e39c-49cf-b06e-d69b98896afa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nemo llm finetune --help llama31_8b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece26938-ebd1-43bc-b843-34c42e353182",
   "metadata": {},
   "source": [
    "```python\n",
    "...\n",
    "╭─ Pre-loaded entrypoint factories, run with --factory ────────────────────────╮\n",
    "│ baichuan2_7b               \u001b]8;id=453595;file:///opt/NeMo-Run/nemo_run/cli/api.py#L236\u001b\\nemo.collections.llm.r…\u001b]8;;\u001b\\                   │\n",
    "│ chatglm3_6b                \u001b]8;id=447150;file:///opt/NeMo-Run/nemo_run/cli/api.py#L236\u001b\\nemo.collections.llm.r…\u001b]8;;\u001b\\                   │\n",
    "│ deepseek_v2                \u001b]8;id=826757;file:///opt/NeMo-Run/nemo_run/cli/api.py#L108\u001b\\nemo.collections.llm.r…\u001b]8;;\u001b\\                   │\n",
    "│ deepseek_v2_lite           \u001b]8;id=169312;file:///opt/NeMo-Run/nemo_run/cli/api.py#L107\u001b\\nemo.collections.llm.r…\u001b]8;;\u001b\\                   │\n",
    "│ deepseek_v3                \u001b]8;id=602877;file:///opt/NeMo-Run/nemo_run/cli/api.py#L88\u001b\\nemo.collections.llm.r…\u001b]8;;\u001b\\                    │\n",
    "│ e5_340m                    \u001b]8;id=586107;file:///opt/NeMo-Run/nemo_run/cli/api.py#L46\u001b\\nemo.collections.llm.r…\u001b]8;;\u001b\\                    │\n",
    "│ gemma2_2b                  \u001b]8;id=885420;file:///opt/NeMo-Run/nemo_run/cli/api.py#L173\u001b\\nemo.collections.llm.r…\u001b]8;;\u001b\\                   │\n",
    "...\n",
    "│ llama3_8b                  \u001b]8;id=606922;file:///opt/NeMo-Run/nemo_run/cli/api.py#L247\u001b\\nemo.collections.llm.r…\u001b]8;;\u001b\\                   │\n",
    "│ llama3_70b                 \u001b]8;id=740427;file:///opt/NeMo-Run/nemo_run/cli/api.py#L251\u001b\\nemo.collections.llm.r…\u001b]8;;\u001b\\                   │\n",
    "│ llama31_8b                 \u001b]8;id=231473;file:///opt/NeMo-Run/nemo_run/cli/api.py#L246\u001b\\nemo.collections.llm.r…\u001b]8;;\u001b\\          \n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d71a6-7827-4536-9ff8-a9f1866bc592",
   "metadata": {},
   "source": [
    "### Getting Started With NeMo Run \n",
    "\n",
    "NeMo Run is a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across various computing environments. NeMo Run has three core responsibilities: [Configuration](https://github.com/NVIDIA-NeMo/Run/blob/main/docs/source/guides/configuration.md), [Execution](https://github.com/NVIDIA-NeMo/Run/blob/main/docs/source/guides/execution.md), and [Management](https://github.com/NVIDIA-NeMo/Run/blob/main/docs/source/guides/management.md).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c8f2e-ac57-4548-b80f-96510308fd0c",
   "metadata": {},
   "source": [
    " #### Finetuning Custom Summarization Dataset with NeMo Run \n",
    "\n",
    "One of the main benefits of NeMo-Run is that it decouples configuration and execution, allowing the reuse of predefined executors and simply changing the recipe. [Important reasons](https://github.com/NVIDIA-NeMo/Run/blob/main/docs/source/guides/why-use-nemo-run.md) why we used NeMo Run are that it provides `Flexibility`, `Modularity`, `Reproducibility`, and `Organization`. To get started with Finetuning: \n",
    "- We need to set up your [Hugging Face token](https://huggingface.co/docs/hub/en/security-tokens) to enable the automatic conversion of the model from Hugging Face.\n",
    "- Configure the Recipe by taking 2 steps: 1) Convert the checkpoint from Hugging Face to NeMo. 2) Run fine-tuning using the converted checkpoint from step 1. We will accomplish this using a NeMo-Run experiment, which allows us to define these two tasks and execute them sequentially with ease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbd325-4f54-4e21-99c6-ebbd4056f249",
   "metadata": {},
   "source": [
    "Log in with your token via huggingface-cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f725e13-67ff-4f02-a44c-755f9ddd6aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token \"Add-your-huggingface-token-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc835b03-f92e-4b19-9c9d-d0b3457a1287",
   "metadata": {},
   "source": [
    "To configure the Recipe, we will write a Python file (llama3_1_8b.py) to pull the Llama 3-1-8B checkpoint from Hugging Face and convert it to NeMo format via the NeMo Run experiment. First, we need to set up the NeMo cache path to store the checkpoint. By default, NeMo stores the checkpoint here: `NEMO_MODELS_CACHE=/root/.cache/nemo/models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc8756-4318-4de1-b81a-92813de2b98d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NEMO_MODELS_CACHE\"] = \"/workspace/model/\"\n",
    "os.environ[\"NEMO_MODELS_CACHE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369da628-071d-4f01-9a2d-46c139d5dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama3_1_8b.py\n",
    "from nemo.collections import llm\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    llm.import_ckpt(\n",
    "       model=llm.LlamaModel(config=llm.Llama31Config8B()),\n",
    "        source=\"hf://meta-llama/Meta-Llama-3.1-8B\",\n",
    "        overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416096b0-7408-4f0b-abf6-b424db69d37f",
   "metadata": {},
   "source": [
    "Run the script to pull the Llama 3-1-8B checkpoint from Hugging Face and convert it to NeMo format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ebe268-f847-4e04-99de-769be1ae2667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!torchrun llama3_1_8b.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4a99fe-034b-4bcc-bcea-9940c138df1b",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```python\n",
    "...\n",
    "\n",
    "[NeMo I 2025-07-27 19:36:37 nemo_logging:393] Successfully saved checkpoint from iteration       0 to /workspace/model/meta-llama/Meta-Llama-3.1-8B\n",
    "[NeMo I 2025-07-27 19:36:38 nemo_logging:393] Async finalization time took 10.174 s\n",
    "Converted Llama model to Nemo, model saved to /workspace/model/meta-llama/Meta-Llama-3.1-8B in torch.bfloat16.\n",
    " $NEMO_MODELS_CACHE=/workspace/model \n",
    "Imported Checkpoint\n",
    "├── context/\n",
    "│   ├── artifacts/\n",
    "│   │   └── generation_config.json\n",
    "│   ├── nemo_tokenizer/\n",
    "│   │   ├── special_tokens_map.json\n",
    "│   │   ├── tokenizer.json\n",
    "│   │   └── tokenizer_config.json\n",
    "│   ├── io.json\n",
    "│   └── model.yaml\n",
    "└── weights/\n",
    "    ├── .metadata\n",
    "    ├── __0_0.distcp\n",
    "    ├── __0_1.distcp\n",
    "    ├── common.pt\n",
    "    └── metadata.json\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aed70b-1a2c-401f-8ca4-9eedaebc1684",
   "metadata": {},
   "source": [
    "Run the functions to configure the recipe and local executor. Note that we set the PEFT scheme (peft_scheme) to LoRA. If you intend to perform a full fine-tuning, you can set it to None `(peft_scheme=None)`. [PEFT](https://arxiv.org/abs/2305.16742) allows fine-tuning a small number of (extra) model parameters instead of all the model's parameters, and this significantly decreases the computational and storage costs. One way to implement PEFT is to adopt the Low-Rank Adaptation (LoRA) technique. Lora makes fine-tuning more efficient by greatly reducing the number of trainable parameters for downstream tasks. It does this by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture. According to the [authors of LoRA](https://arxiv.org/abs/2106.09685), aside from reducing the number of trainable parameters by 10k times, it also reduces the GPU consumption by 3x, thus delivering high throughput with no inference latency.\n",
    "\n",
    "<center><img src=\"images/lora-arch.png\" height=\"400\" width=\"600\"  /></center>\n",
    "<center> LoRA Reparametrization and Weight Merging. <a href=\"https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\"> View source</a> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca918c1b-697c-468e-be3a-41ec52ea2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo_run as run\n",
    "from nemo.collections import llm\n",
    "\n",
    "def configure_recipe(nodes: int = 1, gpus_per_node: int = 1):\n",
    "    recipe = llm.llama31_8b.finetune_recipe(\n",
    "        num_nodes=nodes,\n",
    "        num_gpus_per_node=gpus_per_node,\n",
    "        peft_scheme='lora',\n",
    "    )\n",
    "    return recipe\n",
    "\n",
    "def local_executor_torchrun(devices: int = 1) -> run.LocalExecutor:\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\")\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d972d-7260-4c0a-8f21-98e6b6a9d846",
   "metadata": {},
   "source": [
    "Instantiate the recipe and make sure you set the gpus_per_node as expected. In our case, we set the value to a GPU, 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f679c75-35d8-401d-b2a3-f068fc4d96b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = configure_recipe(gpus_per_node=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300fe3b-8927-4ef9-bcc7-05d7775f21e8",
   "metadata": {},
   "source": [
    "##### Define Custom Data Source \n",
    "\n",
    "From the previous notebook, we preprocessed the SAMSum summarization dataset for the `FineTuningDataModule` and `ChatDataModule` objects. To use the `FineTuningDataModule` object, replace the `recipe.data` value in the cell below with the code snippet below.\n",
    "\n",
    "```python\n",
    " recipe.data = run.Config( llm.FineTuningDataModule,\n",
    "   dataset_root=\"../data/SAMSum/finetune_module/\",\n",
    "   seq_length=2048, #512,\n",
    "   micro_batch_size=1,\n",
    "   global_batch_size=32, #128\n",
    "                           )\n",
    "```\n",
    "For the fine-tuning process, we will use the ChatDataModule format for our custom preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b427cec-688d-492f-831b-e86b2297e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe.data = run.Config(\n",
    "    llm.ChatDataModule,\n",
    "   dataset_root=\"../data/SAMSum/chat_module/\",\n",
    "    seq_length=2048,\n",
    "    micro_batch_size=1,\n",
    "    global_batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc09fab-f3ad-47fc-9981-114b52f4c501",
   "metadata": {},
   "source": [
    "Setting hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d37f13-212a-4374-9cca-216b3e75b79b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recipe.trainer.num_sanity_val_steps = 0\n",
    "\n",
    "# Need to set this to 1 since the default is 2\n",
    "recipe.trainer.strategy.context_parallel_size = 1\n",
    "recipe.trainer.val_check_interval = 100 #0\n",
    "\n",
    "recipe.trainer.limit_val_batches = 0\n",
    "recipe.trainer.max_steps = 100 #40\n",
    "recipe.log.use_datetime_version = False\n",
    "recipe.log.explicit_log_dir = '/workspace/log'\n",
    "recipe.resume.restore_config.path = '/workspace/model/meta-llama/Meta-Llama-3.1-8B/'\n",
    "# adjust other hyperparameters as needed\n",
    "# for example:\n",
    "# recipe.optim.config.lr = 1e-6\n",
    "# recipe.trainer.strategy.tensor_model_parallel_size = 2\n",
    "# recipe.log.ckpt.save_top_k = 3\n",
    "\n",
    "executor = local_executor_torchrun(devices=recipe.trainer.devices)\n",
    "run.run(recipe, executor=executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14475ad8-c012-440b-bfa9-2a48e292b13f",
   "metadata": {},
   "source": [
    "**Likely Output:**\n",
    "```python\n",
    "...\n",
    "\n",
    "Task 0: nemo.collections.llm.api.finetune\n",
    "- Status: RUNNING\n",
    "- Executor: LocalExecutor\n",
    "- Job id: nemo.collections.llm.api.finetune-ztkbrsjbg7b76\n",
    "- Local Directory: /root/.nemo_run/experiments/nemo.collections.llm.api.finetune/nemo.collections.llm.api.finetune_1753648690/nemo.collecti\n",
    "...\n",
    "\n",
    "i.finetune/0 [default0]:[NeMo I 2025-07-27 20:49:05 nemo_logging:393] Successfully saved checkpoint from iteration      99 to /workspace/log/checkpoints/model_name=0--val_loss=0.00-step=99-consumed_samples=3200.0-last.ckpt\n",
    "i.finetune/0 [default0]:[NeMo I 2025-07-27 20:49:05 nemo_logging:393] Async checkpoint save for step 100 (/workspace/log/checkpoints/model_name=0--val_loss=0.00-step=99-consumed_samples=3200.0-last.ckpt) finalized successfully.\n",
    "i.finetune/0 [default0]:[NeMo I 2025-07-27 20:49:05 nemo_logging:393] Async finalization time took 0.256 s\n",
    "i.finetune/0 I0727 20:49:17.888000 7860 torch/distributed/elastic/agent/server/api.py:879] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n",
    "i.finetune/0 I0727 20:49:17.889000 7860 torch/distributed/elastic/agent/server/api.py:932] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish\n",
    "i.finetune/0 I0727 20:49:17.890000 7860 torch/distributed/elastic/agent/server/api.py:946] Done waiting for other agents. Elapsed: 0.00012350082397460938 seconds\n",
    "Job nemo.collections.llm.api.finetune-ztkbrsjbg7b76 finished: SUCCEEDED\n",
    "```\n",
    "<center><img src=\"images/train_output.png\" width=\"700\" height=\"700\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e38f93-8878-4cbc-b2ac-e575ee1f80e5",
   "metadata": {},
   "source": [
    "### Running Inference\n",
    "\n",
    "After successfully training our Llama-3-1-8b checkpoint, we should evaluate the effectiveness of the fine-tuned model. First, as a sanity check, we can quickly evaluate the trained model's performance using NeMo's in-framework inference. To sart with, we need to know the path where adapter checkpoint is saved from the training log:\n",
    "\n",
    "```python\n",
    "...\n",
    "i.finetune/0 [default0]:[NeMo I 2025-07-27 20:49:05 nemo_logging:393] Async checkpoint save for step 100 (/workspace/log/checkpoints/model_name=0--val_loss=0.00-step=99-consumed_samples=3200.0-last.ckpt) finalized successfully.\n",
    "\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c98cc-2182-483e-92fd-8ee12878e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_inference.py\n",
    "\n",
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "import nemo.lightning as nl\n",
    "from nemo.collections.llm import api\n",
    "import torch\n",
    "\n",
    "strategy = nl.MegatronStrategy(\n",
    "    tensor_model_parallel_size=1,\n",
    "    pipeline_model_parallel_size=1,\n",
    "    context_parallel_size=1,\n",
    "    sequence_parallel=False,\n",
    "    setup_optimizers=False,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = nl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    num_nodes=1,\n",
    "    strategy=strategy,\n",
    "    plugins=nl.MegatronMixedPrecision(\n",
    "        precision=\"bf16-mixed\",\n",
    "        params_dtype=torch.bfloat16,\n",
    "        pipeline_dtype=torch.bfloat16,\n",
    "    ),\n",
    ")\n",
    "\n",
    "prompts = [ \"### Instruction: Write a summary of the conversation below. ### Input: Will: hey babe, what do you want for dinner tonight?\\nEmma: gah, don't even worry about it tonight\\nWill: what do you mean? everything ok?\\nEmma: not really, but it's ok, don't worry about cooking though, I'm not hungry\\nWill: Well what time will you be home?\\nEmma: soon, hopefully\\nWill: you sure? Maybe you want me to pick you up?\\nEmma: no no it's alright. I'll be home soon, i'll tell you when I get home.\\nWill: Alright, love you.\\nEmma: love you too.\",\n",
    "\"### Instruction: Write a summary of the conversation below. ### Input: Ollie: Hi , are you in Warsaw\\nJane: yes, just back! Btw are you free for diner the 19th?\\nOllie: nope!\\nJane: and the 18th?\\nOllie: nope, we have this party and you must be there, remember?\\nJane: oh right! i lost my calendar.. thanks for reminding me\\nOllie: we have lunch this week?\\nJane: with pleasure!\\nOllie: friday?\\nJane: ok\\nJane: what do you mean \\\" we don't have any more whisky!\\\" lol..\\nOllie: what!!!\\nJane: you just call me and the all thing i heard was that sentence about whisky... what's wrong with you?\\nOllie: oh oh... very strange! i have to be carefull may be there is some spy in my mobile! lol\\nJane: dont' worry, we'll check on friday.\\nOllie: don't forget to bring some sun with you\\nJane: I can't wait to be in Morocco..\\nOllie: enjoy and see you friday\\nJane: sorry Ollie, i'm very busy, i won't have time for lunch tomorrow, but may be at 6pm after my courses?this trip to Morocco was so nice, but time consuming!\\nOllie: ok for tea!\\nJane: I'm on my way..\\nOllie: tea is ready, did you bring the pastries?\\nJane: I already ate them all... see you in a minute\\nOllie: ok\"\n",
    " ]\n",
    "\n",
    "\n",
    "groundtruth = [\n",
    "     {\"from\": \"Response\", \"value\": \"Emma will be home soon and she will let Will know.\"},\n",
    " {\"from\": \"Response\", \"value\": \"Jane is in Warsaw. Ollie and Jane has a party. Jane lost her calendar. They will get a lunch this week on Friday. Ollie accidentally called Jane and talked about whisky. Jane cancels lunch. They'll meet for a tea at 6 pm.\"}\n",
    "]\n",
    "   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    adapter_checkpoint = \"/workspace/log/checkpoints/model_name=0--val_loss=0.00-step=99-consumed_samples=3200.0-last\"  #\n",
    "    results = api.generate(\n",
    "    path=adapter_checkpoint,\n",
    "    prompts=prompts,\n",
    "    trainer=trainer,\n",
    "    inference_params=CommonInferenceParams(temperature=1, top_k=1, num_tokens_to_generate=100),\n",
    "    text_only=True,\n",
    "    )\n",
    "    nos_of_result= len(results)\n",
    "    for chat, summary in zip(prompts,results):\n",
    "        top_summary = summary.split(\"\\n\")[1]\n",
    "        print (\"Chat History: \", chat, \"\\n\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Summary of the Chat \")\n",
    "        print(\"=\" * 50, '\\n')\n",
    "        print(top_summary)\n",
    "        print(\"=\" * 50, '\\n')\n",
    "    #print(\"Detailed Result: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4eac7-a1b6-4695-ad63-fcb745c92950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!torchrun run_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c3165-37c7-4521-a567-108fe08ef81d",
   "metadata": {},
   "source": [
    "**Likely Output:**\n",
    "\n",
    "```python\n",
    "...\n",
    "[NeMo I 2025-07-27 21:48:34 nemo_logging:393] Adding lora to: module.module.decoder.layers.31.mlp.linear_fc2\n",
    "[NeMo I 2025-07-27 21:48:35 nemo_logging:393] Using <megatron.core.dist_checkpointing.strategies.fully_parallel.FullyParallelLoadStrategyWrapper object at 0x7ff884163cb0> dist-ckpt load strategy.\n",
    "[NeMo I 2025-07-27 21:48:35 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1753652915.032s : Time spent in load_checkpoint: 0.201s\n",
    "static requests: 100%|████████████████████████████| 1/1 [00:09<00:00,  9.85s/it]\n",
    "Chat History:  ### Instruction: Write a summary of the conversation below. ### Input: Will: hey babe, what do you want for dinner tonight?\n",
    "Emma: gah, don't even worry about it tonight\n",
    "Will: what do you mean? everything ok?\n",
    "Emma: not really, but it's ok, don't worry about cooking though, I'm not hungry\n",
    "Will: Well what time will you be home?\n",
    "Emma: soon, hopefully\n",
    "Will: you sure? Maybe you want me to pick you up?\n",
    "Emma: no no it's alright. I'll be home soon, i'll tell you when I get home.\n",
    "Will: Alright, love you.\n",
    "Emma: love you too. \n",
    "\n",
    "==================================================\n",
    "Summary of the Chat \n",
    "================================================== \n",
    "\n",
    "Emma doesn't want Will to cook dinner tonight. She will be home soon.\n",
    "================================================== \n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324db732-fc5d-4420-bef1-903d9a4579fd",
   "metadata": {},
   "source": [
    "After training for 100 steps, we can see that our summarization output is close to the ground truth list in the cell above, but not exact. You can decide to increase the number of steps and modify the values of `temperature`, `top_k`, and `num_tokens_to_generate`. Let's proceed to the next notebook and learn how to apply the prompt engineering approach to prompt our model. Please click the link below.\n",
    "\n",
    "## <center><div style=\"text-align:center; color:#FF0000; border:3px solid red;height:80px;\"> <b><br/> [Next Notebook](prompt-engineering.ipynb) </b> </div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04002beb-a0cb-4f06-9879-fae3d1e93367",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### References\n",
    "- [Quickstart with NeMo-Run](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/quickstart.html)\n",
    "- [NeMo Framework](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html)\n",
    "- [Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models](https://arxiv.org/pdf/2312.12148)\n",
    "- [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685)\n",
    "- [NeMo Run](https://github.com/NVIDIA-NeMo/Run)\n",
    "- [HuggingFace Token](https://huggingface.co/docs/hub/en/security-tokens)\n",
    "- [NeMo 2.0](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html)\n",
    "\n",
    "### Licensing\n",
    "Copyright © 2025 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
